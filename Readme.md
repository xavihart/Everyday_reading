# PAPER READING :cat:

* [PAPER READING <g-emoji class="g-emoji" alias="cat" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f431.png">üê±</g-emoji>](#paper-reading-cat)
   * [Vision](#vision)
      * [3D Perception / NeRF](#3d-perception--nerf)
         * [Spatial Transformer Networks<g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">‚úÖ</g-emoji>](#spatial-transformer-networkswhite_check_mark)
         * [Point-Voxel CNN for Efficient 3D Deep Learning<g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">‚úÖ</g-emoji>](#point-voxel-cnn-for-efficient-3d-deep-learningwhite_check_mark)
         * [GRF<g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">‚úÖ</g-emoji>](#grfwhite_check_mark)
         * [NeX<g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">‚úÖ</g-emoji>](#nexwhite_check_mark)
         * [pixel-NeRF<g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png"> ‚úÖ</g-emoji>](#pixel-nerfwhite_check_mark)
         * [Plenoxels: Radiance Fields without Neural Networks <g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">‚úÖ</g-emoji>](#plenoxels-radiance-fields-without-neural-networks-white_check_mark)
         * [D-NeRF: Neural Radiance Fields for Dynamic Scenes (CVPR2021)<g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">‚úÖ</g-emoji>](#d-nerf-neural-radiance-fields-for-dynamic-scenes-cvpr2021white_check_mark)
         * [PlenOctTree <g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">‚úÖ</g-emoji>](#plenocttree-white_check_mark)
         * [GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis <g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">‚úÖ</g-emoji>](#graf-generative-radiance-fields-for-3d-aware-image-synthesis-white_check_mark)         * [Girrafe: Representing Scenes as Compositional Generative Neural Feature Fields <g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">‚úÖ</g-emoji>](#girrafe-representing-scenes-as-compositional-generative-neural-feature-fields-white_check_mark)
         * [CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields<g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">‚úÖ</g-emoji>](#campari-camera-aware-decomposed-generative-neural-radiance-fieldswhite_check_mark)
         * [GRAM: Generative Radiance Manifolds for 3D-Aware Image Generation (MSRA)](#gram-generative-radiance-manifolds-for-3d-aware-image-generation-msra)
         * [Fast and Explicit Neural View Synthesis (WACV2021)<g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">‚úÖ</g-emoji>](#fast-and-explicit-neural-view-synthesis-wacv2021white_check_mark)
         * [Moving SLAM: Fully Unsupervised Deep Learning in Non-Rigid ScenesÔºà <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Vedaldi%2C+A" rel="nofollow">Andrea Vedaldi</a>Ôºâ](#moving-slam-fully-unsupervised-deep-learning-in-non-rigid-scenes-andrea-vedaldi)
         * [Unsupervised Discovery of 3D Physical Objects From Video Ôºàjiajun group)](#unsupervised-discovery-of-3d-physical-objects-from-video-jiajun-group)
         * [BANMo: Building Animatable 3D Neural Models from Many Casual Videos](#banmo-building-animatable-3d-neural-models-from-many-casual-videos)
         * [Video Autoencoder: self-supervised disentanglement of static 3D structure and motion ÔºàXiaolong Group, ICCV-2021Ôºâ<g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">‚úÖ</g-emoji>](#video-autoencoder-self-supervised-disentanglement-of-static-3d-structure-and-motion-xiaolong-group-iccv-2021white_check_mark)
         * [Neural Radiance Flow for 4D View Synthesis and Video Processing ÔºàJosh Group, ICCV2021Ôºâ](#neural-radiance-flow-for-4d-view-synthesis-and-video-processing-josh-group-iccv2021)
      * [ViT pretrain](#vit-pretrain)
      * [Video Understanding / Motion Analysis](#video-understanding--motion-analysis)
         * [Learning Motion Priors for 4D Human Body Capture in 3D Scenes](#learning-motion-priors-for-4d-human-body-capture-in-3d-scenes)
         * [Object-Centric Learning with Slot Attention (NIPS 2020)](#object-centric-learning-with-slot-attention-nips-2020)
         * [Unsupervised Discovery of Object Radiance Fields (Jiajun group 2021)](#unsupervised-discovery-of-object-radiance-fields-jiajun-group-2021)
      * [Unsupervised Learning](#unsupervised-learning)
      * [GAN](#gan)
         * [GAN-Supervised Dense Visual Alignment (Junyan Zhu 2021.12 arxiv)](#gan-supervised-dense-visual-alignment-junyan-zhu-202112-arxiv)
   * [NLP](#nlp)
      * [Syntactic Learning](#syntactic-learning)
         * [Enhancing Machine Translation with Dependency-Aware Self-Attention <g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">‚úÖ</g-emoji>](#enhancing-machine-translation-with-dependency-aware-self-attention-white_check_mark)
      * [Machine Translation](#machine-translation)
   * [Multi-modal](#multi-modal)
      * [Video, Audio, Text](#video-audio-text)
         * [One-shot Talking Face Generation from Single-speaker Audio-Visual Correlation Learning <g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">‚úÖ</g-emoji>](#one-shot-talking-face-generation-from-single-speaker-audio-visual-correlation-learning-white_check_mark)
   * [Adversarial Training &amp; OoD &amp; Causal Inference](#adversarial-training--ood--causal-inference)
      * [Adversarial Training](#adversarial-training)
         * [Geometry-aware Instance-reweighted Adversarial Training](#geometry-aware-instance-reweighted-adversarial-training)
      * [OoD](#ood)
         * [OoD-Bench](#ood-bench)
         * [DecAug](#decaug)
         * [AIL (CVPR2021)](#ail-cvpr2021)
   * [Robot Learning](#robot-learning)
      * [RL-based](#rl-based)
         * [CURL: Contrastive Unsupervised Representations for Reinforcement Learning <g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">‚úÖ</g-emoji>](#curl-contrastive-unsupervised-representations-for-reinforcement-learning-white_check_mark)
         * [Dynamics Learning with Cascaded Variational Inference for Multi-Step Manipulation (CoRL 2019) <g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">‚úÖ</g-emoji>](#dynamics-learning-with-cascaded-variational-inference-for-multi-step-manipulation-corl-2019-white_check_mark)
      * [Dynamic-based](#dynamic-based)
         * [(ICML 2018, DeepMind)Graph Networks as Learnable Physics Engines for Inference and Control](#icml-2018-deepmindgraph-networks-as-learnable-physics-engines-for-inference-and-control)
         * [(ICML 2020, DeepMind) Learning to Simulate Complex Physics with Graph Networks<g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">‚úÖ</g-emoji>](#icml-2020-deepmind-learning-to-simulate-complex-physics-with-graph-networkswhite_check_mark)
         * [Realtime Trajectory Smoothing with Neural Nets](#realtime-trajectory-smoothing-with-neural-nets)
         * [Garment Similarity Network (GarNet): A Continuous Perception Robotic Approach for Predicting Shapes and Visually Perceived Weights of Unseen Garments](#garment-similarity-network-garnet-a-continuous-perception-robotic-approach-for-predicting-shapes-and-visually-perceived-weights-of-unseen-garments)
         * [(ICLR2021 outstanding paper awards, DeepMind) Learning Mesh Based Simulation with Graph Neural Networks](#iclr2021-outstanding-paper-awards-deepmind-learning-mesh-based-simulation-with-graph-neural-networks)
         * [Garment Similarity Network (GarNet): A Continuous Perception Robotic Approach for Predicting Shapes and Visually Perceived Weights of Unseen Garments](#garment-similarity-network-garnet-a-continuous-perception-robotic-approach-for-predicting-shapes-and-visually-perceived-weights-of-unseen-garments-1)
   * [Multi-modal](#multi-modal-1)
      * [Generalized Learning / Intuitive Physics / Cognition](#generalized-learning--intuitive-physics--cognition)
* [Conference Tutorials / Lectures](#conference-tutorials--lectures)
         * [MIT Deep Learning Seminar highlighting recent work (January 2020) by Animesh Garg](#mit-deep-learning-seminar-highlighting-recent-work-january-2020-by-animesh-garg)
         * [RSS 2020 KeyNotes of Cognitive Core for Robot Learning by Josh Tenenbaum](#rss-2020-keynotes-of-cognitive-core-for-robot-learning-by-josh-tenenbaum)
         * [Generative Neural Scene Representationsfor 3D-Aware Image Synthesis](#generative-neural-scene-representationsfor-3d-aware-image-synthesis)

## Vision

### 3D Perception / NeRF

#### Spatial Transformer Networks:white_check_mark:

#### Point-Voxel CNN for Efficient 3D Deep Learning:white_check_mark:

#### GRF:white_check_mark:

#### NeX:white_check_mark:

#### pixel-NeRF:white_check_mark:

#### Plenoxels: Radiance Fields without Neural Networks :white_check_mark:

  - No Neural Nets, 100 times faster on training 
  - Data Structure: Sparse Voxel = (Spherical Harmonic Basis + Opacity)
  - Rendering: Neural Rendering, Trilinear Interpolation
  - Optimization: reconstruction loss + total variation loss (smooth variation)
  - Key: the paper demonstrate that : the soa performance on novel view synthesis of NeRF lies on the Neural Rendering Reconstruction, but not on NN.

#### D-NeRF: Neural Radiance Fields for Dynamic Scenes (CVPR2021):white_check_mark:

#### PlenOctTree :white_check_mark:

  - real-time nerf rendering, OCTree structure
  - using spherical harmonic basis to encode view-dependent effects, which is much faster than conventional NeRF rendering where NN inference is needed 
  - learn F: (x, y, z) -> (SHs, $\sigma$)





#### GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis :white_check_mark:





#### Girrafe: Representing Scenes as Compositional Generative Neural Feature Fields :white_check_mark:





#### CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields:white_check_mark:

#### GRAM: Generative Radiance Manifolds for 3D-Aware Image Generation (MSRA)

#### Fast and Explicit Neural View Synthesis (WACV2021):white_check_mark:

- generalized novel view syn, faster rendering(amortized rendering)Ôºå voxel-based
- (3, h, w)-(2D-UNet)-(c, h, w)-(inverse projection)-(c, ds, hs, ws) - (3D UNet)- (4, ds, h, w)
- why inverse projection / reshape ?
- my opinion : voxel-based -> fixed resolution, not flexible enough







#### Moving SLAM: Fully Unsupervised Deep Learning in Non-Rigid ScenesÔºà [Andrea Vedaldi](https://arxiv.org/search/cs?searchtype=author&query=Vedaldi%2C+A)Ôºâ





#### Unsupervised Discovery of 3D Physical Objects From Video Ôºàjiajun group)





#### BANMo: Building Animatable 3D Neural Models from Many Casual Videos









#### Video Autoencoder: self-supervised disentanglement of static 3D structure and motion ÔºàXiaolong Group, ICCV-2021Ôºâ:white_check_mark:

- 





#### Neural Radiance Flow for 4D View Synthesis and Video Processing ÔºàJosh Group, ICCV2021Ôºâ






### ViT pretrain



### Video Understanding / Motion Analysis
#### Learning Motion Priors for 4D Human Body Capture in 3D Scenes
  - pose estimation, smoothing
#### Object-Centric Learning with Slot Attention (NIPS 2020)
#### Unsupervised Discovery of Object Radiance Fields (Jiajun group 2021)



### Unsupervised Learning





### GAN

#### GAN-Supervised Dense Visual Alignment (Junyan Zhu 2021.12 arxiv)
  -  Image/Video Align : learn a transformation to congeal a image 





## NLP

### Syntactic Learning
#### Enhancing Machine Translation with Dependency-Aware Self-Attention :white_check_mark:
  - NLP, syntactically-enhanced Transformer

### Machine Translation











## Multi-modal

### Video, Audio, Text

#### One-shot Talking Face Generation from Single-speaker Audio-Visual Correlation Learning :white_check_mark:

- audio-visual correlation transformer (input : phonemes + facial key point-based motion field)
- really complex system...
  - phonemes label 
  - acoustic feature (from some traditional extractor)
  - structural feature (from pretrained face detector)
  - head pose predictor
  - adversarial training
  - ...



## Adversarial Training & OoD & Causal Inference

### Adversarial Training

####  Geometry-aware Instance-reweighted Adversarial Training

### OoD

#### OoD-Bench

#### DecAug



#### AIL (CVPR2021)




## Robot Learning

### RL-based
#### CURL: Contrastive Unsupervised Representations for Reinforcement Learning :white_check_mark:
  -  RL from pixel, contrastive learning, moco style

#### Dynamics Learning with Cascaded Variational Inference for Multi-Step Manipulation (CoRL 2019) :white_check_mark:

  - sequential task for robots, subgoals 
  - C : latent effect space (subgoals), Z: latent motion space (actions), ~N(0, 1)
  - two levels
    - high level task : subgoals -> target states(s_t) : p(s_t|s, c)
    - low level task : actions -> subgoals a~p(s_t|s, c, z) 
- learn from task-agnostic manipulations
- Reinforcement and Imitation Learning for Diverse Visuomotor Skills (2018)
- 

### Dynamic-based





#### (ICML 2018, DeepMind)Graph Networks as Learnable Physics Engines for Inference and Control





#### (ICML 2020, DeepMind) Learning to Simulate Complex Physics with Graph Networks:white_check_mark:

- GNS framework to learn complex particle-based dynamics system
  - dpi is specialized on each scene, but one single GNS can work well on many scenes



#### Realtime Trajectory Smoothing with Neural Nets





#### Garment Similarity Network (GarNet): A Continuous Perception Robotic Approach for Predicting Shapes and Visually Perceived Weights of Unseen Garments







#### (ICLR2021 outstanding paper awards, DeepMind) Learning Mesh Based Simulation with Graph Neural Networks 









#### Garment Similarity Network (GarNet): A Continuous Perception Robotic Approach for Predicting Shapes and Visually Perceived Weights of Unseen Garments








## Multi-modal

- Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks (2019)
- Soundify: Matching Sound Effects to Video ÔºàCMUÔºâ



### Generalized Learning / Intuitive Physics / Cognition

- MarioNette: Self-Supervised Sprite Learning (NIPS2021) :white_check_mark:
  - RGB scene decompose, sprite learning
- Grounding Physical Concepts of Objects and Events Through Dynamic Visual Reasoning (ICLR2021)
- iGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday Household Tasks
- Point-Voxel CNN for Efficient 3D Deep Learning
- SFV: Reinforcement Learning of Physical Skills from Videos :white_check_mark:
  - video -> pose -> imitation for robot
- Modeling Expectation Violation in Intuitive Physics with Coarse Probabilistic Object Representations (2019 Josh Group)




# Conference Tutorials / Lectures

#### MIT Deep Learning Seminar highlighting recent work (January 2020) by Animesh Garg
  - Generalization of Robot for different bu similar tasks
    - current paradigm: e.g. Deep RL, env + action + states + reward, sampling inefficient and unstable
    - current paradigm: Visuo-motor skills, need to carefully design the robot model for specifc scene
    - current paradigm, DRL + Visuo-motor, policy -> velocity, impedence of end effector
  - Heuristics often beats RL
    - How can human guide help with robot training (Human in-loop training): Imitation from human heuristics, off-policy RL DDPG
  -  Sequential Tasks
     - Grasp + Oriented-tasks
     - Multi-step reasoning
       - **CAVIN, hierachical planning** (http://ai.stanford.edu/blog/cavin/)

  - Compositional Planning (Generalized)
       - states (e.g. videos) -> program (pick blue, lift bule, place blue, pick red, lift red ...), instead of just ouput actions using end-to-end way
       - task graphs
  - Data for Robotics
    - largest data source : direcly from videos
    - current robot data is much fewer than that of CV and NLP
      - **experts needs show, not label**
      - RoboTurk (? set games for human to collect human manipulations?)


- CVPR 2021 Workshop on Learning from Instructional Videos

- CVPR workshop on solution to general robot by Pieter Abbeel
  - How to effeciently learn generalized policy from pixels ?
  - How to bring pre-traininig into RL
#### RSS 2020 KeyNotes of Cognitive Core for Robot Learning by Josh Tenenbaum







#### Generative Neural Scene Representationsfor 3D-Aware Image Synthesis

  - By Andreas Geiger from MPI
    - We need control over the image generation process ! 
    - Goal:
      - generate photorealistic images
      - control individual objects, appearance, pose, size in 3D
      - novel view image synthesis
      - train from raw images without pose information
    - representation used ?
      - voxel (PlanoticGAN, iccv2019)
        - sample -> voxel 3D -> render
        - low fidelity
      - deep-voxel (HOLOGAN, iccv2019)
        - sample -> deep voxel(feature) -> NN + render
      - generative radiance field
        - sample -> radiance field -> render
    - GRAF: 
    - Train  
      
      -  ![image-20220102204848958](C:\Users\38433\AppData\Roaming\Typora\typora-user-images\image-20220102204848958.png)
    - GIRAFFE : compositional  3d-aware representation
    - CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields
    





#### Towards Causal Representation Learning

- By Bengio, 2020

- Gap between current ML and HUMAN-LEVEL AI

  - causality, inference, explain things happening

- Compositionality helps IID AND OOD Generalization

  - Systematic Generalization
  - E.G. driving in a new city, science fiction (combination of seen objects)

- Conscious Processing Helps Humans Deal With OOD

  - FACED WITH NEW SITUATIONS, HUMANS CAN CALL UP ON-THE-FLY KNOWLEDGES AND USE THEM TO INFERENCE AND MAKE SOLUTIONS
  - AGENTS FACE NON-STATIONARITIES

- TWO Systems : Sys1 vs Sys2 (By Daniel Kahneman)

  - Sup:

    - ```
      Israeli-American psychologist and Nobel Laureate Daniel Kahneman is the founding father of modern behavioral economics. His work has influenced how we see thinking, decisions, risk, and even happiness.
      
      In Thinking, Fast and Slow, his ‚Äúintellectual memoir,‚Äù he shows us in his own words some of his enormous body of work.
      
      Part of that body includes a description of the ‚Äúmachinery of ‚Ä¶ thought,‚Äù which divides the brain into two agents, called System 1 and System 2, which ‚Äúrespectively produce fast and slow thinking.‚Äù For our purposes, these can also be thought of as intuitive and deliberate thought.
      
      
      ```

    - System1: intuitive, simple, one-step logic, fast, on-effort (current DL)

    - System2: conscious, complex, logic, slow, hard (future DL)

- SOME sys2 inductive priors:

  - Sparse factor graph in space of high-level semantic variables
  - Semantic variables are causal: agents, intentions, controllable objects
  - 








